{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "to9-3E9-xves",
        "outputId": "c50cd298-8392-4346-92aa-d5d1c348b817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n",
            "Found existing installation: spacy 3.8.7\n",
            "Uninstalling spacy-3.8.7:\n",
            "  Successfully uninstalled spacy-3.8.7\n",
            "Found existing installation: accelerate 1.8.1\n",
            "Uninstalling accelerate-1.8.1:\n",
            "  Successfully uninstalled accelerate-1.8.1\n",
            "Found existing installation: peft 0.16.0\n",
            "Uninstalling peft-0.16.0:\n",
            "  Successfully uninstalled peft-0.16.0\n",
            "Found existing installation: fastai 2.7.19\n",
            "Uninstalling fastai-2.7.19:\n",
            "  Successfully uninstalled fastai-2.7.19\n",
            "Found existing installation: sentence-transformers 4.1.0\n",
            "Uninstalling sentence-transformers-4.1.0:\n",
            "  Successfully uninstalled sentence-transformers-4.1.0\n",
            "Collecting numpy==1.26.3\n",
            "  Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, torch_geometric\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.3 torch_geometric-2.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f2e86f8070e542b38911d80d634b1e32"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Clean up conflicting packages\n",
        "!pip uninstall -y thinc spacy accelerate peft fastai sentence-transformers\n",
        "\n",
        "# Install core requirements\n",
        "!pip install numpy==1.26.3 --upgrade torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hp1iGkbGxte",
        "outputId": "f2cbe427-aa57-4354-c0a7-eea86b31b2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gnnintegrated'...\n",
            "remote: Enumerating objects: 2821, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 2821 (delta 98), reused 18 (delta 18), pack-reused 2674 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2821/2821), 1.06 MiB | 18.33 MiB/s, done.\n",
            "Resolving deltas: 100% (1748/1748), done.\n",
            "/content/gnnintegrated\n",
            "/content/gnnintegrated\n"
          ]
        }
      ],
      "source": [
        "# First clone the repository\n",
        "!git clone https://github.com/rishabharizona/gnnintegrated.git\n",
        "%cd gnnintegrated\n",
        "\n",
        "# Verify current directory\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L_4d7iBHLXd",
        "outputId": "8e74ec46-a927-4eec-b1e3-af0c07965fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 10:00:41--  https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip\n",
            "Resolving wjdcloud.blob.core.windows.net (wjdcloud.blob.core.windows.net)... 20.60.131.4\n",
            "Connecting to wjdcloud.blob.core.windows.net (wjdcloud.blob.core.windows.net)|20.60.131.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20237244 (19M) [application/zip]\n",
            "Saving to: ‘diversity_emg.zip’\n",
            "\n",
            "diversity_emg.zip   100%[===================>]  19.30M  3.06MB/s    in 7.5s    \n",
            "\n",
            "2025-07-11 10:00:50 (2.59 MB/s) - ‘diversity_emg.zip’ saved [20237244/20237244]\n",
            "\n",
            "Archive:  diversity_emg.zip\n",
            "   creating: emg/\n",
            "   creating: emg/20/\n",
            "  inflating: emg/20/1_raw_data_11-41_22.03.16.txt  \n",
            "  inflating: emg/20/2_raw_data_11-43_22.03.16.txt  \n",
            "   creating: emg/35/\n",
            "  inflating: emg/35/2_raw_data_10-05_13.04.16.txt  \n",
            "  inflating: emg/35/1_raw_data_10-03_13.04.16.txt  \n",
            "   creating: emg/09/\n",
            "  inflating: emg/09/1_raw_data_12-41_23.03.16.txt  \n",
            "  inflating: emg/09/2_raw_data_12-43_23.03.16.txt  \n",
            "   creating: emg/15/\n",
            "  inflating: emg/15/2_raw_data_08-51_13.04.16.txt  \n",
            "  inflating: emg/15/1_raw_data_08-49_13.04.16.txt  \n",
            "   creating: emg/22/\n",
            "  inflating: emg/22/1_raw_data_12-37_28.03.16.txt  \n",
            "  inflating: emg/22/2_raw_data_12-39_28.03.16.txt  \n",
            "   creating: emg/13/\n",
            "  inflating: emg/13/1_raw_data_13-26_21.03.16.txt  \n",
            "  inflating: emg/13/2_raw_data_13-29_21.03.16.txt  \n",
            "   creating: emg/30/\n",
            "  inflating: emg/30/1_raw_data_09-49_21.03.16.txt  \n",
            "  inflating: emg/30/2_raw_data_09-50_21.03.16.txt  \n",
            "   creating: emg/01/\n",
            "  inflating: emg/01/2_raw_data_13-13_22.03.16.txt  \n",
            "  inflating: emg/01/1_raw_data_13-12_22.03.16.txt  \n",
            "   creating: emg/28/\n",
            "  inflating: emg/28/1_raw_data_12-10_15.04.16.txt  \n",
            "  inflating: emg/28/2_raw_data_12-11_15.04.16.txt  \n",
            "  inflating: emg/README.txt          \n",
            "   creating: emg/34/\n",
            "  inflating: emg/34/2_raw_data_10-53_07.04.16.txt  \n",
            "  inflating: emg/34/1_raw_data_10-51_07.04.16.txt  \n",
            "   creating: emg/06/\n",
            "  inflating: emg/06/2_raw_data_10-40_11.04.16.txt  \n",
            "  inflating: emg/06/1_raw_data_10-38_11.04.16.txt  \n",
            "   creating: emg/25/\n",
            "  inflating: emg/25/2_raw_data_14-53_24.04.16.txt  \n",
            "  inflating: emg/25/1_raw_data_14-51_24.04.16.txt  \n",
            "   creating: emg/26/\n",
            "  inflating: emg/26/2_raw_data_10-23_29.03.16.txt  \n",
            "  inflating: emg/26/1_raw_data_10-22_29.03.16.txt  \n",
            "   creating: emg/36/\n",
            "  inflating: emg/36/1_raw_data_13-03_15.04.16.txt  \n",
            "  inflating: emg/36/2_raw_data_13-04_15.04.16.txt  \n",
            "   creating: emg/04/\n",
            "  inflating: emg/04/1_raw_data_18-02_24.04.16.txt  \n",
            "  inflating: emg/04/2_raw_data_18-03_24.04.16.txt  \n",
            "   creating: emg/14/\n",
            "  inflating: emg/14/2_raw_data_09-51_15.04.16.txt  \n",
            "  inflating: emg/14/1_raw_data_09-50_15.04.16.txt  \n",
            "   creating: emg/02/\n",
            "  inflating: emg/02/2_raw_data_14-21_22.03.16.txt  \n",
            "  inflating: emg/02/1_raw_data_14-19_22.03.16.txt  \n",
            "   creating: emg/27/\n",
            "  inflating: emg/27/1_raw_data_12-19_06.04.16.txt  \n",
            "  inflating: emg/27/2_raw_data_12-20_06.04.16.txt  \n",
            "   creating: emg/17/\n",
            "  inflating: emg/17/2_raw_data_11-20_23.03.16.txt  \n",
            "  inflating: emg/17/1_raw_data_11-19_23.03.16.txt  \n",
            "  inflating: emg/emg_x.npy           \n",
            "   creating: emg/03/\n",
            "  inflating: emg/03/1_raw_data_09-32_11.04.16.txt  \n",
            "  inflating: emg/03/2_raw_data_09-34_11.04.16.txt  \n",
            "   creating: emg/31/\n",
            "  inflating: emg/31/2_raw_data_11-16_11.04.16.txt  \n",
            "  inflating: emg/31/1_raw_data_11-15_11.04.16.txt  \n",
            "   creating: emg/11/\n",
            "  inflating: emg/11/1_raw_data_13-11_18.03.16.txt  \n",
            "  inflating: emg/11/2_raw_data_13-13_18.03.16.txt  \n",
            "   creating: emg/21/\n",
            "  inflating: emg/21/2_raw_data_20-30_24.04.16.txt  \n",
            "  inflating: emg/21/1_raw_data_20-28_24.04.16.txt  \n",
            "   creating: emg/10/\n",
            "  inflating: emg/10/1_raw_data_11-08_21.03.16.txt  \n",
            "  inflating: emg/10/2_raw_data_11-10_21.03.16.txt  \n",
            "   creating: emg/05/\n",
            "  inflating: emg/05/2_raw_data_10-29_30.03.16.txt  \n",
            "  inflating: emg/05/1_raw_data_10-28_30.03.16.txt  \n",
            "   creating: emg/19/\n",
            "  inflating: emg/19/1_raw_data_12-10_26.04.16.txt  \n",
            "  inflating: emg/19/2_raw_data_12-11_26.04.16.txt  \n",
            "   creating: emg/08/\n",
            "  inflating: emg/08/1_raw_data_12-14_23.03.16.txt  \n",
            "  inflating: emg/08/2_raw_data_12-16_23.03.16.txt  \n",
            "   creating: emg/12/\n",
            "  inflating: emg/12/2_raw_data_11-36_28.03.16.txt  \n",
            "  inflating: emg/12/1_raw_data_11-35_28.03.16.txt  \n",
            "   creating: emg/16/\n",
            "  inflating: emg/16/2_raw_data_12-14_25.04.16.txt  \n",
            "  inflating: emg/16/1_raw_data_12-12_25.04.16.txt  \n",
            "   creating: emg/24/\n",
            "  inflating: emg/24/1_raw_data_10-16_12.04.16.txt  \n",
            "  inflating: emg/24/2_raw_data_10-17_12.04.16.txt  \n",
            "   creating: emg/33/\n",
            "  inflating: emg/33/2_raw_data_09-50_12.04.16.txt  \n",
            "  inflating: emg/33/1_raw_data_09-49_12.04.16.txt  \n",
            "   creating: emg/07/\n",
            "  inflating: emg/07/2_raw_data_18-50_22.03.16.txt  \n",
            "  inflating: emg/07/1_raw_data_18-48_22.03.16.txt  \n",
            "   creating: emg/32/\n",
            "  inflating: emg/32/2_raw_data_12-06_27.04.16.txt  \n",
            "  inflating: emg/32/1_raw_data_12-04_27.04.16.txt  \n",
            "   creating: emg/18/\n",
            "  inflating: emg/18/1_raw_data_12-35_21.03.16.txt  \n",
            "  inflating: emg/18/2_raw_data_12-37_21.03.16.txt  \n",
            "   creating: emg/23/\n",
            "  inflating: emg/23/1_raw_data_13-18_05.04.16.txt  \n",
            "  inflating: emg/23/2_raw_data_13-19_05.04.16.txt  \n",
            "   creating: emg/29/\n",
            "  inflating: emg/29/2_raw_data_10-18_15.04.16.txt  \n",
            "  inflating: emg/29/1_raw_data_10-17_15.04.16.txt  \n",
            "  inflating: emg/emg_y.npy           \n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip\n",
        "!unzip diversity_emg.zip && mv emg data/\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p ./data/train_output/act/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyOsUKDiHLTx",
        "outputId": "03729434-eac4-4861-8cb7-7e8d089ff051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  diversity_emg.zip\n",
            "   creating: emg/\n",
            "   creating: emg/20/\n",
            "  inflating: emg/20/1_raw_data_11-41_22.03.16.txt  \n",
            "  inflating: emg/20/2_raw_data_11-43_22.03.16.txt  \n",
            "   creating: emg/35/\n",
            "  inflating: emg/35/2_raw_data_10-05_13.04.16.txt  \n",
            "  inflating: emg/35/1_raw_data_10-03_13.04.16.txt  \n",
            "   creating: emg/09/\n",
            "  inflating: emg/09/1_raw_data_12-41_23.03.16.txt  \n",
            "  inflating: emg/09/2_raw_data_12-43_23.03.16.txt  \n",
            "   creating: emg/15/\n",
            "  inflating: emg/15/2_raw_data_08-51_13.04.16.txt  \n",
            "  inflating: emg/15/1_raw_data_08-49_13.04.16.txt  \n",
            "   creating: emg/22/\n",
            "  inflating: emg/22/1_raw_data_12-37_28.03.16.txt  \n",
            "  inflating: emg/22/2_raw_data_12-39_28.03.16.txt  \n",
            "   creating: emg/13/\n",
            "  inflating: emg/13/1_raw_data_13-26_21.03.16.txt  \n",
            "  inflating: emg/13/2_raw_data_13-29_21.03.16.txt  \n",
            "   creating: emg/30/\n",
            "  inflating: emg/30/1_raw_data_09-49_21.03.16.txt  \n",
            "  inflating: emg/30/2_raw_data_09-50_21.03.16.txt  \n",
            "   creating: emg/01/\n",
            "  inflating: emg/01/2_raw_data_13-13_22.03.16.txt  \n",
            "  inflating: emg/01/1_raw_data_13-12_22.03.16.txt  \n",
            "   creating: emg/28/\n",
            "  inflating: emg/28/1_raw_data_12-10_15.04.16.txt  \n",
            "  inflating: emg/28/2_raw_data_12-11_15.04.16.txt  \n",
            "  inflating: emg/README.txt          \n",
            "   creating: emg/34/\n",
            "  inflating: emg/34/2_raw_data_10-53_07.04.16.txt  \n",
            "  inflating: emg/34/1_raw_data_10-51_07.04.16.txt  \n",
            "   creating: emg/06/\n",
            "  inflating: emg/06/2_raw_data_10-40_11.04.16.txt  \n",
            "  inflating: emg/06/1_raw_data_10-38_11.04.16.txt  \n",
            "   creating: emg/25/\n",
            "  inflating: emg/25/2_raw_data_14-53_24.04.16.txt  \n",
            "  inflating: emg/25/1_raw_data_14-51_24.04.16.txt  \n",
            "   creating: emg/26/\n",
            "  inflating: emg/26/2_raw_data_10-23_29.03.16.txt  \n",
            "  inflating: emg/26/1_raw_data_10-22_29.03.16.txt  \n",
            "   creating: emg/36/\n",
            "  inflating: emg/36/1_raw_data_13-03_15.04.16.txt  \n",
            "  inflating: emg/36/2_raw_data_13-04_15.04.16.txt  \n",
            "   creating: emg/04/\n",
            "  inflating: emg/04/1_raw_data_18-02_24.04.16.txt  \n",
            "  inflating: emg/04/2_raw_data_18-03_24.04.16.txt  \n",
            "   creating: emg/14/\n",
            "  inflating: emg/14/2_raw_data_09-51_15.04.16.txt  \n",
            "  inflating: emg/14/1_raw_data_09-50_15.04.16.txt  \n",
            "   creating: emg/02/\n",
            "  inflating: emg/02/2_raw_data_14-21_22.03.16.txt  \n",
            "  inflating: emg/02/1_raw_data_14-19_22.03.16.txt  \n",
            "   creating: emg/27/\n",
            "  inflating: emg/27/1_raw_data_12-19_06.04.16.txt  \n",
            "  inflating: emg/27/2_raw_data_12-20_06.04.16.txt  \n",
            "   creating: emg/17/\n",
            "  inflating: emg/17/2_raw_data_11-20_23.03.16.txt  \n",
            "  inflating: emg/17/1_raw_data_11-19_23.03.16.txt  \n",
            "  inflating: emg/emg_x.npy           \n",
            "   creating: emg/03/\n",
            "  inflating: emg/03/1_raw_data_09-32_11.04.16.txt  \n",
            "  inflating: emg/03/2_raw_data_09-34_11.04.16.txt  \n",
            "   creating: emg/31/\n",
            "  inflating: emg/31/2_raw_data_11-16_11.04.16.txt  \n",
            "  inflating: emg/31/1_raw_data_11-15_11.04.16.txt  \n",
            "   creating: emg/11/\n",
            "  inflating: emg/11/1_raw_data_13-11_18.03.16.txt  \n",
            "  inflating: emg/11/2_raw_data_13-13_18.03.16.txt  \n",
            "   creating: emg/21/\n",
            "  inflating: emg/21/2_raw_data_20-30_24.04.16.txt  \n",
            "  inflating: emg/21/1_raw_data_20-28_24.04.16.txt  \n",
            "   creating: emg/10/\n",
            "  inflating: emg/10/1_raw_data_11-08_21.03.16.txt  \n",
            "  inflating: emg/10/2_raw_data_11-10_21.03.16.txt  \n",
            "   creating: emg/05/\n",
            "  inflating: emg/05/2_raw_data_10-29_30.03.16.txt  \n",
            "  inflating: emg/05/1_raw_data_10-28_30.03.16.txt  \n",
            "   creating: emg/19/\n",
            "  inflating: emg/19/1_raw_data_12-10_26.04.16.txt  \n",
            "  inflating: emg/19/2_raw_data_12-11_26.04.16.txt  \n",
            "   creating: emg/08/\n",
            "  inflating: emg/08/1_raw_data_12-14_23.03.16.txt  \n",
            "  inflating: emg/08/2_raw_data_12-16_23.03.16.txt  \n",
            "   creating: emg/12/\n",
            "  inflating: emg/12/2_raw_data_11-36_28.03.16.txt  \n",
            "  inflating: emg/12/1_raw_data_11-35_28.03.16.txt  \n",
            "   creating: emg/16/\n",
            "  inflating: emg/16/2_raw_data_12-14_25.04.16.txt  \n",
            "  inflating: emg/16/1_raw_data_12-12_25.04.16.txt  \n",
            "   creating: emg/24/\n",
            "  inflating: emg/24/1_raw_data_10-16_12.04.16.txt  \n",
            "  inflating: emg/24/2_raw_data_10-17_12.04.16.txt  \n",
            "   creating: emg/33/\n",
            "  inflating: emg/33/2_raw_data_09-50_12.04.16.txt  \n",
            "  inflating: emg/33/1_raw_data_09-49_12.04.16.txt  \n",
            "   creating: emg/07/\n",
            "  inflating: emg/07/2_raw_data_18-50_22.03.16.txt  \n",
            "  inflating: emg/07/1_raw_data_18-48_22.03.16.txt  \n",
            "   creating: emg/32/\n",
            "  inflating: emg/32/2_raw_data_12-06_27.04.16.txt  \n",
            "  inflating: emg/32/1_raw_data_12-04_27.04.16.txt  \n",
            "   creating: emg/18/\n",
            "  inflating: emg/18/1_raw_data_12-35_21.03.16.txt  \n",
            "  inflating: emg/18/2_raw_data_12-37_21.03.16.txt  \n",
            "   creating: emg/23/\n",
            "  inflating: emg/23/1_raw_data_13-18_05.04.16.txt  \n",
            "  inflating: emg/23/2_raw_data_13-19_05.04.16.txt  \n",
            "   creating: emg/29/\n",
            "  inflating: emg/29/2_raw_data_10-18_15.04.16.txt  \n",
            "  inflating: emg/29/1_raw_data_10-17_15.04.16.txt  \n",
            "  inflating: emg/emg_y.npy           \n"
          ]
        }
      ],
      "source": [
        "!unzip diversity_emg.zip\n",
        "!mkdir -p ./data/emg\n",
        "!mv emg/* ./data/emg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HpYjNHNrSMz",
        "outputId": "a4583199-6cc6-42c1-d0bf-37d286de6fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastdtw) (1.26.3)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp311-cp311-linux_x86_64.whl size=542084 sha256=2ceebea8b2e8d04d05cc29c7413f813c68d375a8dffab6a2574cd047bb0d6f7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85pZXTIJUuJx",
        "outputId": "b4dee67b-3b1d-4fce-dd3a-31c93b1c501a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752228100.267320     984 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752228100.277783     984 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "GNN modules successfully imported\n",
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 1.26.3\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm: diversify\n",
            "alpha: 1.0\n",
            "alpha1: 1.0\n",
            "batch_size: 64\n",
            "beta1: 0.5\n",
            "checkpoint_freq: 100\n",
            "local_epoch: 3\n",
            "max_epoch: 2\n",
            "lr: 0.01\n",
            "lr_decay1: 1.0\n",
            "lr_decay2: 1.0\n",
            "weight_decay: 0.001\n",
            "dropout: 0.5\n",
            "label_smoothing: 0.0\n",
            "bottleneck: 256\n",
            "classifier: linear\n",
            "dis_hidden: 256\n",
            "layer: bn\n",
            "model_size: median\n",
            "lam: 0.0\n",
            "latent_domain_num: None\n",
            "domain_num: 0\n",
            "data_file: \n",
            "dataset: emg\n",
            "data_dir: ./data/\n",
            "task: cross_people\n",
            "test_envs: [0]\n",
            "N_WORKERS: 4\n",
            "automated_k: True\n",
            "curriculum: True\n",
            "CL_PHASE_EPOCHS: 2\n",
            "enable_shap: True\n",
            "resume: None\n",
            "debug_mode: False\n",
            "use_gnn: True\n",
            "gnn_hidden_dim: 128\n",
            "gnn_output_dim: 256\n",
            "gnn_lr: 0.001\n",
            "gnn_weight_decay: 0.0\n",
            "gnn_pretrain_epochs: 2\n",
            "gpu_id: 0\n",
            "seed: 0\n",
            "output: ./train_output\n",
            "old: False\n",
            "steps_per_epoch: 10000000000\n",
            "select_position: {'emg': [0]}\n",
            "select_channel: {'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list: {'emg': 1000}\n",
            "act_people: {'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "input_shape: (8, 1, 200)\n",
            "num_classes: 6\n",
            "grid_size: 10\n",
            "lambda_cls: 1.0\n",
            "lambda_dis: 0.0001\n",
            "max_grad_norm: 1.0\n",
            "gnn_layers: 1\n",
            "use_tcn: True\n",
            "lstm_hidden_size: 128\n",
            "lstm_layers: 1\n",
            "bidirectional: True\n",
            "lstm_dropout: 0.0\n",
            "optimizer: adam\n",
            "domain_adv_weight: 0.1\n",
            "jitter_scale: 0.5\n",
            "scaling_std: 0.5\n",
            "warp_ratio: 0.5\n",
            "aug_prob: 0.9\n",
            "early_stopping_patience: 30\n",
            "adv_weight: 0.1\n",
            "\n",
            "Using device: cuda\n",
            "Using default latent_domain_num: 5\n",
            "Curriculum settings: Phases=3, Epochs per phase: [2, 2, 2], Difficulties: [0.2, 0.5, 0.8]\n",
            "Class distribution: {0: 866, 1: 829, 2: 856, 3: 885, 4: 864, 5: 879}\n",
            "Class distribution: {0: 274, 1: 279, 2: 288, 3: 290, 4: 291, 5: 282}\n",
            "Dataset sizes: Train=4143, Val=1036, Target=1704\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Using PyGDataLoader for GNN data\n",
            "\n",
            "Running automated K estimation...\n",
            "Using GNN for feature extraction\n",
            "K=2: Silhouette=0.6042, DBI=0.6760, CH=2932.7550, Combined=1.4002\n",
            "K=3: Silhouette=0.3810, DBI=1.3165, CH=2170.4727, Combined=1.0827\n",
            "K=4: Silhouette=0.3116, DBI=1.3637, CH=1794.9057, Combined=0.9510\n",
            "K=5: Silhouette=0.2255, DBI=1.5232, CH=1535.6598, Combined=0.8463\n",
            "K=6: Silhouette=0.2442, DBI=1.5372, CH=1367.9259, Combined=0.8003\n",
            "K=7: Silhouette=0.2380, DBI=1.5271, CH=1241.0312, Combined=0.7610\n",
            "K=8: Silhouette=0.2292, DBI=1.5271, CH=1148.2804, Combined=0.7309\n",
            "K=9: Silhouette=0.2301, DBI=1.5419, CH=1064.3451, Combined=0.7055\n",
            "K=10: Silhouette=0.1828, DBI=1.5375, CH=993.5250, Combined=0.6726\n",
            "[INFO] Optimal K determined as 2 (Combined Score: 1.4002)\n",
            "Using automated latent_domain_num (K): 2\n",
            "Adjusted batch size: 64\n",
            "Initialized domain alignment (weight: 0.5)\n",
            "Detected actual feature dimension: 1600\n",
            "Warning: No skip connection layer found in featurizer\n",
            "\n",
            "===== Initializing GNN Feature Extractor =====\n",
            "Bottleneck: 256 -> 256\n",
            "\n",
            "==== GNN Pretraining (2 epochs) ====\n",
            "Created reconstruction head with output dim: 8\n",
            "GNN Pretrain Epoch 1/2: Loss 0.0250\n",
            "GNN Pretrain Epoch 2/2: Loss 0.0003\n",
            "GNN pretraining complete\n",
            "Teacher model initialized with current student architecture\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Added domain adversarial training (weight: 0.1)\n",
            "\n",
            "======== ROUND 0 ========\n",
            "\n",
            "Curriculum Stage 1/3\n",
            "Difficulty: 0.2, Epochs: 2\n",
            "Expanding curriculum to 828 easiest samples\n",
            "Curriculum Stage 1: Threshold=0.20, Samples=828/4143 (20.0%), Avg Difficulty=0.115\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.258925        \n",
            "1                2.021331        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.237530         1.215541         2.225374        \n",
            "1                0.908290         1.213072         0.896159        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {1: 171, 0: 657}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Initializing projection head for 256 features\n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "0                1.072246         0.000011         7.334399         34.541063        14.671815        16.901408        12.417623       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "1                1.084885         0.000017         5.498070         51.207729        15.637066        16.490610        12.263048       \n",
            "\n",
            "Calculating h-divergence...\n",
            " H-Divergence: 0.7453, Domain Classifier Acc: 0.6863\n",
            "\n",
            "======== ROUND 1 ========\n",
            "\n",
            "Curriculum Stage 2/3\n",
            "Difficulty: 0.5, Epochs: 2\n",
            "Expanding curriculum to 828 easiest samples\n",
            "Filtered to 828 high-confidence samples (confidence ≥ 0.60)\n",
            "Curriculum Stage 2: Threshold=0.50, Samples=828/4143 (20.0%), Avg Difficulty=0.137\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.170643        \n",
            "1                1.844283        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.887401         1.168651         0.875715        \n",
            "1                0.846454         1.139643         0.835058        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {0: 655, 1: 173}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "2                1.003738         0.000000         7.249846         36.231884        16.409266        16.960094        12.274735       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "3                1.041804         0.000000         5.399605         30.676329        16.409266        16.314554        12.728621       \n",
            "\n",
            "🎯 Final Target Accuracy: 16.3146\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "Collecting SHAP background data for GNN...\n",
            "Loader type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
            "Background sample node features shape: torch.Size([8, 1, 200])\n",
            "Evaluation batch node features shape: torch.Size([80, 1, 200])\n",
            "Using first sample as background for GNN\n",
            "Created evaluation batch with 10 graphs\n",
            "Computing SHAP values...\n",
            "/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_pytorch.py:255: UserWarning: unrecognized nn.Module: Flatten\n",
            "  warnings.warn(f\"unrecognized nn.Module: {module_type}\")\n",
            "DeepExplainer failed: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.. Using KernelExplainer\n",
            "100%|##########| 80/80 [00:21<00:00,  3.72it/s]\n",
            "SHAP values shape: (80, 200, 6)\n",
            "Aggregated SHAP values shape: (80, 200)\n",
            "Original SHAP values shape: (80, 200, 6)\n",
            "Original X_eval shape: (80, 1, 200)\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./train_output/shap_gnn_sample.html\n",
            "✅ Saved summary plot: ./train_output/shap_summary.png\n",
            "✅ Saved signal overlay: ./train_output/shap_overlay.png\n",
            "✅ Saved SHAP heatmap: ./train_output/shap_heatmap.png\n",
            "Evaluating SHAP impact...\n",
            "✅ Saved SHAP values to: ./train_output/shap_values.npy\n",
            "[SHAP] Accuracy Drop: 60.0000\n",
            "[SHAP] Flip Rate: 0.6000\n",
            "[SHAP] Confidence Δ: 0.0070\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 3.9358\n",
            "[SHAP] Coherence: 0.0264\n",
            "[SHAP] Channel Variance: 0.0000\n",
            "[SHAP] Temporal Entropy: 4.8460\n",
            "[SHAP] Mutual Info: 0.0180\n",
            "[SHAP] PCA Alignment: 0.2473\n",
            "[SHAP] Jaccard (top-10): 0.3333\n",
            "[SHAP] Kendall's Tau: 0.2331\n",
            "[SHAP] Cosine Similarity: 0.4666\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./train_output/shap_4d_scatter.html\n",
            "[Surface] SHAP shape: (80, 6, 1, 200)\n",
            "✅ Saved SHAP surface: ./train_output/shap_4d_surface.html\n",
            "Generating confusion matrix...\n",
            "✅ Confusion matrix saved\n",
            "✅ Training metrics plot saved\n",
            "✅ Domain discrepancy plot saved\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data_dir ./data/ --task cross_people --test_envs 0 --dataset emg --algorithm diversify --alpha1 1.0 --alpha 1.0 --lam 0.0 --local_epoch 3 --max_epoch 2 --lr 0.0001 --output ./train_output --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 2 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yviBcJPGK8N9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed219009-ca61-4385-8e8e-2e14ab87525a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752228342.251513    2291 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752228342.257847    2291 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "GNN modules successfully imported\n",
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 1.26.3\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm: diversify\n",
            "alpha: 10.0\n",
            "alpha1: 0.1\n",
            "batch_size: 64\n",
            "beta1: 0.5\n",
            "checkpoint_freq: 100\n",
            "local_epoch: 4\n",
            "max_epoch: 3\n",
            "lr: 0.01\n",
            "lr_decay1: 1.0\n",
            "lr_decay2: 1.0\n",
            "weight_decay: 0.001\n",
            "dropout: 0.5\n",
            "label_smoothing: 0.0\n",
            "bottleneck: 256\n",
            "classifier: linear\n",
            "dis_hidden: 256\n",
            "layer: bn\n",
            "model_size: median\n",
            "lam: 0.0\n",
            "latent_domain_num: None\n",
            "domain_num: 0\n",
            "data_file: \n",
            "dataset: emg\n",
            "data_dir: ./data/\n",
            "task: cross_people\n",
            "test_envs: [1]\n",
            "N_WORKERS: 4\n",
            "automated_k: True\n",
            "curriculum: True\n",
            "CL_PHASE_EPOCHS: 4\n",
            "enable_shap: True\n",
            "resume: None\n",
            "debug_mode: False\n",
            "use_gnn: True\n",
            "gnn_hidden_dim: 128\n",
            "gnn_output_dim: 256\n",
            "gnn_lr: 0.001\n",
            "gnn_weight_decay: 0.0\n",
            "gnn_pretrain_epochs: 3\n",
            "gpu_id: 0\n",
            "seed: 0\n",
            "output: ./data/train_output1\n",
            "old: False\n",
            "steps_per_epoch: 10000000000\n",
            "select_position: {'emg': [0]}\n",
            "select_channel: {'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list: {'emg': 1000}\n",
            "act_people: {'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "input_shape: (8, 1, 200)\n",
            "num_classes: 6\n",
            "grid_size: 10\n",
            "lambda_cls: 1.0\n",
            "lambda_dis: 0.0001\n",
            "max_grad_norm: 1.0\n",
            "gnn_layers: 1\n",
            "use_tcn: True\n",
            "lstm_hidden_size: 128\n",
            "lstm_layers: 1\n",
            "bidirectional: True\n",
            "lstm_dropout: 0.0\n",
            "optimizer: adam\n",
            "domain_adv_weight: 0.1\n",
            "jitter_scale: 0.5\n",
            "scaling_std: 0.5\n",
            "warp_ratio: 0.5\n",
            "aug_prob: 0.9\n",
            "early_stopping_patience: 30\n",
            "adv_weight: 0.1\n",
            "\n",
            "Using device: cuda\n",
            "Using default latent_domain_num: 5\n",
            "Curriculum settings: Phases=3, Epochs per phase: [4, 4, 4], Difficulties: [0.2, 0.5, 0.8]\n",
            "Class distribution: {0: 843, 1: 823, 2: 836, 3: 866, 4: 856, 5: 845}\n",
            "Class distribution: {0: 297, 1: 285, 2: 308, 3: 309, 4: 299, 5: 316}\n",
            "Dataset sizes: Train=4055, Val=1014, Target=1814\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Using PyGDataLoader for GNN data\n",
            "\n",
            "Running automated K estimation...\n",
            "Using GNN for feature extraction\n",
            "K=2: Silhouette=0.6029, DBI=0.6702, CH=2787.1866, Combined=1.3566\n",
            "K=3: Silhouette=0.3679, DBI=1.3177, CH=2057.9795, Combined=1.0456\n",
            "K=4: Silhouette=0.3200, DBI=1.3400, CH=1704.2340, Combined=0.9268\n",
            "K=5: Silhouette=0.2345, DBI=1.5364, CH=1465.3708, Combined=0.8271\n",
            "K=6: Silhouette=0.2288, DBI=1.5729, CH=1291.9994, Combined=0.7725\n",
            "K=7: Silhouette=0.2213, DBI=1.5536, CH=1169.5790, Combined=0.7345\n",
            "K=8: Silhouette=0.2145, DBI=1.5191, CH=1085.6248, Combined=0.7087\n",
            "K=9: Silhouette=0.2244, DBI=1.5719, CH=1005.8094, Combined=0.6856\n",
            "K=10: Silhouette=0.1762, DBI=1.5514, CH=940.9377, Combined=0.6547\n",
            "[INFO] Optimal K determined as 2 (Combined Score: 1.3566)\n",
            "Using automated latent_domain_num (K): 2\n",
            "Adjusted batch size: 64\n",
            "Initialized domain alignment (weight: 0.5)\n",
            "Detected actual feature dimension: 1600\n",
            "Warning: No skip connection layer found in featurizer\n",
            "\n",
            "===== Initializing GNN Feature Extractor =====\n",
            "Bottleneck: 256 -> 256\n",
            "\n",
            "==== GNN Pretraining (3 epochs) ====\n",
            "Created reconstruction head with output dim: 8\n",
            "GNN Pretrain Epoch 1/3: Loss 0.0254\n",
            "GNN Pretrain Epoch 2/3: Loss 0.0003\n",
            "GNN Pretrain Epoch 3/3: Loss 0.0003\n",
            "GNN pretraining complete\n",
            "Teacher model initialized with current student architecture\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Added domain adversarial training (weight: 0.1)\n",
            "\n",
            "======== ROUND 0 ========\n",
            "\n",
            "Curriculum Stage 1/3\n",
            "Difficulty: 0.2, Epochs: 4\n",
            "Expanding curriculum to 811 easiest samples\n",
            "Curriculum Stage 1: Threshold=0.20, Samples=811/4055 (20.0%), Avg Difficulty=0.126\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.306189        \n",
            "1                2.043043        \n",
            "2                1.873694        \n",
            "3                1.771044        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.682077         1.231641         1.669760        \n",
            "1                0.870904         1.162781         0.859276        \n",
            "2                0.862630         1.138614         0.851244        \n",
            "3                0.889388         1.154412         0.877844        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {0: 696, 1: 115}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Initializing projection head for 256 features\n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "0                1.225912         0.000016         7.164304         42.540074        16.469428        15.104741        12.997313       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "1                0.920342         0.000009         5.843143         42.786683        20.118343        17.475193        12.437824       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "2                0.983367         0.000000         7.358218         49.321825        16.469428        16.372657        12.488350       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "3                0.920910         0.000009         4.216800         56.350185        17.850099        18.687982        12.415037       \n",
            "\n",
            "Calculating h-divergence...\n",
            " H-Divergence: 0.7496, Domain Classifier Acc: 0.6874\n",
            "\n",
            "======== ROUND 1 ========\n",
            "\n",
            "Curriculum Stage 2/3\n",
            "Difficulty: 0.5, Epochs: 4\n",
            "Expanding curriculum to 811 easiest samples\n",
            "Curriculum Stage 2: Threshold=0.50, Samples=811/4055 (20.0%), Avg Difficulty=0.093\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.936317        \n",
            "1                1.685512        \n",
            "2                1.606479        \n",
            "3                1.512819        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.863385         1.140722         0.851978        \n",
            "1                0.860280         1.126560         0.849014        \n",
            "2                0.851229         1.135730         0.839872        \n",
            "3                0.853748         1.136959         0.842379        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {0: 728, 1: 83}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "4                1.050904         0.000000         4.703704         36.004932        11.341223        10.915105        12.562896       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "5                0.910890         0.000003         3.906122         35.141800        15.581854        16.262404        12.956117       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "6                1.048778         0.000001         4.131196         35.388409        18.343195        16.041896        12.388387       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "7                0.938801         0.000001         3.923470         35.511714        17.455621        14.884234        12.430034       \n",
            "\n",
            "======== ROUND 2 ========\n",
            "\n",
            "Curriculum Stage 3/3\n",
            "Difficulty: 0.8, Epochs: 4\n",
            "Filtered to 1329 high-confidence samples (confidence ≥ 0.50)\n",
            "Balanced curriculum: 3 domains, 1257 samples (419 per domain)\n",
            "Curriculum Stage 3: Threshold=0.80, Samples=1257/4055 (31.0%), Avg Difficulty=0.294\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.956867        \n",
            "1                1.826105        \n",
            "2                1.734756        \n",
            "3                1.712772        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.605314         1.211916         1.593195        \n",
            "1                1.081892         1.188910         1.070003        \n",
            "2                1.067712         1.187444         1.055838        \n",
            "3                1.054576         1.190069         1.042675        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {0: 1167, 1: 90}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "8                4.511884         0.000000         4.657051         34.924423        11.439842        11.907387        15.945887       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "9                327.775635       0.000019         4.233415         24.423230        15.877712        15.821389        15.295425       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "10               6495.464844      0.000000         5.644282         32.458234        18.145957        18.853363        15.708467       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "11               108833.898438    0.000000         5.867104         37.311058        17.652860        19.570011        15.484312       \n",
            "\n",
            "🎯 Final Target Accuracy: 19.5700\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "Collecting SHAP background data for GNN...\n",
            "Loader type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
            "Background sample node features shape: torch.Size([8, 1, 200])\n",
            "Evaluation batch node features shape: torch.Size([80, 1, 200])\n",
            "Using first sample as background for GNN\n",
            "Created evaluation batch with 10 graphs\n",
            "Computing SHAP values...\n",
            "/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_pytorch.py:255: UserWarning: unrecognized nn.Module: Flatten\n",
            "  warnings.warn(f\"unrecognized nn.Module: {module_type}\")\n",
            "DeepExplainer failed: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.. Using KernelExplainer\n",
            "100%|##########| 80/80 [00:20<00:00,  3.96it/s]\n",
            "SHAP values shape: (80, 200, 6)\n",
            "Aggregated SHAP values shape: (80, 200)\n",
            "Original SHAP values shape: (80, 200, 6)\n",
            "Original X_eval shape: (80, 1, 200)\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output1/shap_gnn_sample.html\n",
            "✅ Saved summary plot: ./data/train_output1/shap_summary.png\n",
            "✅ Saved signal overlay: ./data/train_output1/shap_overlay.png\n",
            "✅ Saved SHAP heatmap: ./data/train_output1/shap_heatmap.png\n",
            "Evaluating SHAP impact...\n",
            "✅ Saved SHAP values to: ./data/train_output1/shap_values.npy\n",
            "[SHAP] Accuracy Drop: 40.0000\n",
            "[SHAP] Flip Rate: 0.4000\n",
            "[SHAP] Confidence Δ: 0.0235\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 3.9084\n",
            "[SHAP] Coherence: -0.0039\n",
            "[SHAP] Channel Variance: 0.0000\n",
            "[SHAP] Temporal Entropy: 4.8623\n",
            "[SHAP] Mutual Info: 0.0100\n",
            "[SHAP] PCA Alignment: 0.1847\n",
            "[SHAP] Jaccard (top-10): 0.4286\n",
            "[SHAP] Kendall's Tau: 0.3174\n",
            "[SHAP] Cosine Similarity: 0.5147\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output1/shap_4d_scatter.html\n",
            "[Surface] SHAP shape: (80, 6, 1, 200)\n",
            "✅ Saved SHAP surface: ./data/train_output1/shap_4d_surface.html\n",
            "Generating confusion matrix...\n",
            "✅ Confusion matrix saved\n",
            "✅ Training metrics plot saved\n",
            "✅ Domain discrepancy plot saved\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data_dir ./data/ --task cross_people --test_envs 1 --dataset emg --algorithm diversify --alpha1 0.1 --alpha 10.0 --lam 0.0 --local_epoch 4 --max_epoch 3 --lr 0.01 --output ./data/train_output1 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 4 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-M8JJG58K8GM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91eeb148-ab0c-4728-a91c-97a25ade1e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752229141.336393    6774 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752229141.342557    6774 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "GNN modules successfully imported\n",
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 1.26.3\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm: diversify\n",
            "alpha: 21.5\n",
            "alpha1: 0.5\n",
            "batch_size: 64\n",
            "beta1: 0.5\n",
            "checkpoint_freq: 100\n",
            "local_epoch: 4\n",
            "max_epoch: 2\n",
            "lr: 0.01\n",
            "lr_decay1: 1.0\n",
            "lr_decay2: 1.0\n",
            "weight_decay: 0.001\n",
            "dropout: 0.5\n",
            "label_smoothing: 0.0\n",
            "bottleneck: 256\n",
            "classifier: linear\n",
            "dis_hidden: 256\n",
            "layer: bn\n",
            "model_size: median\n",
            "lam: 0.0\n",
            "latent_domain_num: None\n",
            "domain_num: 0\n",
            "data_file: \n",
            "dataset: emg\n",
            "data_dir: ./data/\n",
            "task: cross_people\n",
            "test_envs: [1]\n",
            "N_WORKERS: 4\n",
            "automated_k: True\n",
            "curriculum: True\n",
            "CL_PHASE_EPOCHS: 5\n",
            "enable_shap: True\n",
            "resume: None\n",
            "debug_mode: False\n",
            "use_gnn: True\n",
            "gnn_hidden_dim: 128\n",
            "gnn_output_dim: 256\n",
            "gnn_lr: 0.001\n",
            "gnn_weight_decay: 0.0\n",
            "gnn_pretrain_epochs: 5\n",
            "gpu_id: 0\n",
            "seed: 0\n",
            "output: ./data/train_output2\n",
            "old: False\n",
            "steps_per_epoch: 10000000000\n",
            "select_position: {'emg': [0]}\n",
            "select_channel: {'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list: {'emg': 1000}\n",
            "act_people: {'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "input_shape: (8, 1, 200)\n",
            "num_classes: 6\n",
            "grid_size: 10\n",
            "lambda_cls: 1.0\n",
            "lambda_dis: 0.0001\n",
            "max_grad_norm: 1.0\n",
            "gnn_layers: 1\n",
            "use_tcn: True\n",
            "lstm_hidden_size: 128\n",
            "lstm_layers: 1\n",
            "bidirectional: True\n",
            "lstm_dropout: 0.0\n",
            "optimizer: adam\n",
            "domain_adv_weight: 0.1\n",
            "jitter_scale: 0.5\n",
            "scaling_std: 0.5\n",
            "warp_ratio: 0.5\n",
            "aug_prob: 0.9\n",
            "early_stopping_patience: 30\n",
            "adv_weight: 0.1\n",
            "\n",
            "Using device: cuda\n",
            "Using default latent_domain_num: 5\n",
            "Curriculum settings: Phases=3, Epochs per phase: [5, 5, 5], Difficulties: [0.2, 0.5, 0.8]\n",
            "Class distribution: {0: 843, 1: 823, 2: 836, 3: 866, 4: 856, 5: 845}\n",
            "Class distribution: {0: 297, 1: 285, 2: 308, 3: 309, 4: 299, 5: 316}\n",
            "Dataset sizes: Train=4055, Val=1014, Target=1814\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Using PyGDataLoader for GNN data\n",
            "\n",
            "Running automated K estimation...\n",
            "Using GNN for feature extraction\n",
            "K=2: Silhouette=0.6029, DBI=0.6702, CH=2787.1866, Combined=1.3566\n",
            "K=3: Silhouette=0.3679, DBI=1.3177, CH=2057.9795, Combined=1.0456\n",
            "K=4: Silhouette=0.3200, DBI=1.3400, CH=1704.2340, Combined=0.9268\n",
            "K=5: Silhouette=0.2345, DBI=1.5364, CH=1465.3708, Combined=0.8271\n",
            "K=6: Silhouette=0.2288, DBI=1.5729, CH=1291.9994, Combined=0.7725\n",
            "K=7: Silhouette=0.2213, DBI=1.5536, CH=1169.5790, Combined=0.7345\n",
            "K=8: Silhouette=0.2145, DBI=1.5191, CH=1085.6248, Combined=0.7087\n",
            "K=9: Silhouette=0.2244, DBI=1.5719, CH=1005.8094, Combined=0.6856\n",
            "K=10: Silhouette=0.1762, DBI=1.5514, CH=940.9377, Combined=0.6547\n",
            "[INFO] Optimal K determined as 2 (Combined Score: 1.3566)\n",
            "Using automated latent_domain_num (K): 2\n",
            "Adjusted batch size: 64\n",
            "Initialized domain alignment (weight: 0.5)\n",
            "Detected actual feature dimension: 1600\n",
            "Warning: No skip connection layer found in featurizer\n",
            "\n",
            "===== Initializing GNN Feature Extractor =====\n",
            "Bottleneck: 256 -> 256\n",
            "\n",
            "==== GNN Pretraining (5 epochs) ====\n",
            "Created reconstruction head with output dim: 8\n",
            "GNN Pretrain Epoch 1/5: Loss 0.0254\n",
            "GNN Pretrain Epoch 2/5: Loss 0.0003\n",
            "GNN Pretrain Epoch 3/5: Loss 0.0003\n",
            "GNN Pretrain Epoch 4/5: Loss 0.0002\n",
            "GNN Pretrain Epoch 5/5: Loss 0.0002\n",
            "GNN pretraining complete\n",
            "Teacher model initialized with current student architecture\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Added domain adversarial training (weight: 0.1)\n",
            "\n",
            "======== ROUND 0 ========\n",
            "\n",
            "Curriculum Stage 1/3\n",
            "Difficulty: 0.2, Epochs: 5\n",
            "Expanding curriculum to 811 easiest samples\n",
            "Curriculum Stage 1: Threshold=0.20, Samples=811/4055 (20.0%), Avg Difficulty=0.126\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.379833        \n",
            "1                2.154906        \n",
            "2                2.002897        \n",
            "3                1.845069        \n",
            "4                1.725719        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                2.153078         1.200336         2.141074        \n",
            "1                0.869182         1.138090         0.857801        \n",
            "2                0.887314         1.143664         0.875877        \n",
            "3                0.872577         1.128302         0.861294        \n",
            "4                0.871133         1.137887         0.859754        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {0: 708, 1: 103}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Initializing projection head for 256 features\n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "0                1.042477         0.000010         6.649785         46.362515        17.850099        16.207277        12.384919       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "1                0.947352         0.000008         5.822243         51.418002        15.088757        15.766262        12.336013       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "2                0.983105         0.000000         7.108662         22.934649        12.524655        14.222712        12.249867       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "3                0.957775         0.000010         4.198238         42.416769        14.299803        15.656009        12.268146       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "4                0.938730         0.000000         4.586162         51.787916        14.595661        16.207277        12.343478       \n",
            "\n",
            "Calculating h-divergence...\n",
            " H-Divergence: 0.7496, Domain Classifier Acc: 0.6874\n",
            "\n",
            "======== ROUND 1 ========\n",
            "\n",
            "Curriculum Stage 2/3\n",
            "Difficulty: 0.5, Epochs: 5\n",
            "Expanding curriculum to 811 easiest samples\n",
            "Curriculum Stage 2: Threshold=0.50, Samples=811/4055 (20.0%), Avg Difficulty=0.096\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                1.724099        \n",
            "1                1.469793        \n",
            "2                1.366127        \n",
            "3                1.323082        \n",
            "4                1.305521        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                0.986413         1.161886         0.974794        \n",
            "1                0.852841         1.138613         0.841455        \n",
            "2                0.865620         1.158116         0.854039        \n",
            "3                0.852031         1.174114         0.840290        \n",
            "4                0.860476         1.182511         0.848650        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {1: 649, 0: 162}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "5                1.018395         0.000004         3.757318         32.552404        13.806706        14.608600        12.551403       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "6                0.942804         0.000001         3.951085         35.388409        14.299803        15.049614        12.657266       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "7                0.868015         0.000001         3.820344         35.018496        14.694280        15.270121        12.612537       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "8                1.002712         0.000010         3.507151         34.895191        14.694280        15.159868        12.430061       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "9                0.882282         0.000000         4.075044         36.251541        14.891519        15.600882        12.472427       \n",
            "\n",
            "🎯 Final Target Accuracy: 15.6009\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "Collecting SHAP background data for GNN...\n",
            "Loader type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
            "Background sample node features shape: torch.Size([8, 1, 200])\n",
            "Evaluation batch node features shape: torch.Size([80, 1, 200])\n",
            "Using first sample as background for GNN\n",
            "Created evaluation batch with 10 graphs\n",
            "Computing SHAP values...\n",
            "/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_pytorch.py:255: UserWarning: unrecognized nn.Module: Flatten\n",
            "  warnings.warn(f\"unrecognized nn.Module: {module_type}\")\n",
            "DeepExplainer failed: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.. Using KernelExplainer\n",
            "100%|##########| 80/80 [00:21<00:00,  3.65it/s]\n",
            "SHAP values shape: (80, 200, 6)\n",
            "Aggregated SHAP values shape: (80, 200)\n",
            "Original SHAP values shape: (80, 200, 6)\n",
            "Original X_eval shape: (80, 1, 200)\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output2/shap_gnn_sample.html\n",
            "✅ Saved summary plot: ./data/train_output2/shap_summary.png\n",
            "✅ Saved signal overlay: ./data/train_output2/shap_overlay.png\n",
            "✅ Saved SHAP heatmap: ./data/train_output2/shap_heatmap.png\n",
            "Evaluating SHAP impact...\n",
            "✅ Saved SHAP values to: ./data/train_output2/shap_values.npy\n",
            "[SHAP] Accuracy Drop: 70.0000\n",
            "[SHAP] Flip Rate: 0.7000\n",
            "[SHAP] Confidence Δ: 0.0664\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 3.9290\n",
            "[SHAP] Coherence: -0.0007\n",
            "[SHAP] Channel Variance: 0.0000\n",
            "[SHAP] Temporal Entropy: 4.8502\n",
            "[SHAP] Mutual Info: 0.0080\n",
            "[SHAP] PCA Alignment: 0.2330\n",
            "[SHAP] Jaccard (top-10): 0.2500\n",
            "[SHAP] Kendall's Tau: 0.5390\n",
            "[SHAP] Cosine Similarity: 0.6099\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output2/shap_4d_scatter.html\n",
            "[Surface] SHAP shape: (80, 6, 1, 200)\n",
            "✅ Saved SHAP surface: ./data/train_output2/shap_4d_surface.html\n",
            "Generating confusion matrix...\n",
            "✅ Confusion matrix saved\n",
            "✅ Training metrics plot saved\n",
            "✅ Domain discrepancy plot saved\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data_dir ./data/ --task cross_people --test_envs 2 --dataset emg --algorithm diversify --alpha1 0.5 --alpha 21.5 --lam 0.0 --local_epoch 4 --max_epoch 2 --lr 0.01 --output ./data/train_output2 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.3 --automated_k --curriculum --CL_PHASE_EPOCHS 5 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ETVh85e3K78w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a983200-f9ba-4c28-b57d-6837a0a4d6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752228888.810485    5352 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752228888.817038    5352 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "GNN modules successfully imported\n",
            "Environment:\n",
            "\tPython: 3.11.13\n",
            "\tPyTorch: 2.6.0+cu124\n",
            "\tTorchvision: 0.21.0+cu124\n",
            "\tCUDA: 12.4\n",
            "\tCUDNN: 90300\n",
            "\tNumPy: 1.26.3\n",
            "\tPIL: 11.2.1\n",
            "==========================================\n",
            "algorithm: diversify\n",
            "alpha: 0.1\n",
            "alpha1: 5.0\n",
            "batch_size: 64\n",
            "beta1: 0.5\n",
            "checkpoint_freq: 100\n",
            "local_epoch: 3\n",
            "max_epoch: 1\n",
            "lr: 0.01\n",
            "lr_decay1: 1.0\n",
            "lr_decay2: 1.0\n",
            "weight_decay: 0.001\n",
            "dropout: 0.5\n",
            "label_smoothing: 0.0\n",
            "bottleneck: 256\n",
            "classifier: linear\n",
            "dis_hidden: 256\n",
            "layer: bn\n",
            "model_size: median\n",
            "lam: 0.0\n",
            "latent_domain_num: None\n",
            "domain_num: 0\n",
            "data_file: \n",
            "dataset: emg\n",
            "data_dir: ./data/\n",
            "task: cross_people\n",
            "test_envs: [3]\n",
            "N_WORKERS: 4\n",
            "automated_k: True\n",
            "curriculum: True\n",
            "CL_PHASE_EPOCHS: 6\n",
            "enable_shap: True\n",
            "resume: None\n",
            "debug_mode: False\n",
            "use_gnn: True\n",
            "gnn_hidden_dim: 128\n",
            "gnn_output_dim: 256\n",
            "gnn_lr: 0.001\n",
            "gnn_weight_decay: 0.0\n",
            "gnn_pretrain_epochs: 4\n",
            "gpu_id: 0\n",
            "seed: 0\n",
            "output: ./data/train_output3\n",
            "old: False\n",
            "steps_per_epoch: 10000000000\n",
            "select_position: {'emg': [0]}\n",
            "select_channel: {'emg': array([0, 1, 2, 3, 4, 5, 6, 7])}\n",
            "hz_list: {'emg': 1000}\n",
            "act_people: {'emg': [[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 29, 30, 31, 32, 33, 34, 35]]}\n",
            "input_shape: (8, 1, 200)\n",
            "num_classes: 6\n",
            "grid_size: 10\n",
            "lambda_cls: 1.0\n",
            "lambda_dis: 0.0001\n",
            "max_grad_norm: 1.0\n",
            "gnn_layers: 1\n",
            "use_tcn: True\n",
            "lstm_hidden_size: 128\n",
            "lstm_layers: 1\n",
            "bidirectional: True\n",
            "lstm_dropout: 0.0\n",
            "optimizer: adam\n",
            "domain_adv_weight: 0.1\n",
            "jitter_scale: 0.5\n",
            "scaling_std: 0.5\n",
            "warp_ratio: 0.5\n",
            "aug_prob: 0.9\n",
            "early_stopping_patience: 30\n",
            "adv_weight: 0.1\n",
            "\n",
            "Using device: cuda\n",
            "Using default latent_domain_num: 5\n",
            "Curriculum settings: Phases=3, Epochs per phase: [6, 6, 6], Difficulties: [0.2, 0.5, 0.8]\n",
            "Class distribution: {0: 857, 1: 834, 2: 863, 3: 886, 4: 877, 5: 873}\n",
            "Class distribution: {0: 283, 1: 274, 2: 281, 3: 289, 4: 278, 5: 288}\n",
            "Dataset sizes: Train=4152, Val=1038, Target=1693\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Using PyGDataLoader for GNN data\n",
            "\n",
            "Running automated K estimation...\n",
            "Using GNN for feature extraction\n",
            "K=2: Silhouette=0.5931, DBI=0.6814, CH=2835.7528, Combined=1.3680\n",
            "K=3: Silhouette=0.3797, DBI=1.3107, CH=2090.8793, Combined=1.0587\n",
            "K=4: Silhouette=0.2768, DBI=1.4672, CH=1727.6465, Combined=0.9186\n",
            "K=5: Silhouette=0.2377, DBI=1.4881, CH=1491.4041, Combined=0.8372\n",
            "K=6: Silhouette=0.2336, DBI=1.5528, CH=1331.5251, Combined=0.7862\n",
            "K=7: Silhouette=0.2267, DBI=1.5346, CH=1203.7988, Combined=0.7467\n",
            "K=8: Silhouette=0.2346, DBI=1.5399, CH=1103.7403, Combined=0.7185\n",
            "K=9: Silhouette=0.2137, DBI=1.5756, CH=1025.8957, Combined=0.6888\n",
            "K=10: Silhouette=0.1888, DBI=1.5707, CH=953.3300, Combined=0.6610\n",
            "[INFO] Optimal K determined as 2 (Combined Score: 1.3680)\n",
            "Using automated latent_domain_num (K): 2\n",
            "Adjusted batch size: 64\n",
            "Initialized domain alignment (weight: 0.5)\n",
            "Detected actual feature dimension: 1600\n",
            "Warning: No skip connection layer found in featurizer\n",
            "\n",
            "===== Initializing GNN Feature Extractor =====\n",
            "Bottleneck: 256 -> 256\n",
            "\n",
            "==== GNN Pretraining (4 epochs) ====\n",
            "Created reconstruction head with output dim: 8\n",
            "GNN Pretrain Epoch 1/4: Loss 0.0252\n",
            "GNN Pretrain Epoch 2/4: Loss 0.0003\n",
            "GNN Pretrain Epoch 3/4: Loss 0.0003\n",
            "GNN Pretrain Epoch 4/4: Loss 0.0002\n",
            "GNN pretraining complete\n",
            "Teacher model initialized with current student architecture\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Added domain adversarial training (weight: 0.1)\n",
            "\n",
            "======== ROUND 0 ========\n",
            "\n",
            "Curriculum Stage 1/3\n",
            "Difficulty: 0.2, Epochs: 6\n",
            "Expanding curriculum to 830 easiest samples\n",
            "Curriculum Stage 1: Threshold=0.20, Samples=830/4152 (20.0%), Avg Difficulty=0.121\n",
            "\n",
            "==== Feature update ====\n",
            "epoch            class_loss      \n",
            "0                2.343921        \n",
            "1                2.108832        \n",
            "2                1.938520        \n",
            "3                1.815290        \n",
            "4                1.685255        \n",
            "5                1.556401        \n",
            "\n",
            "==== Latent domain characterization ====\n",
            "epoch            total_loss       dis_loss         ent_loss        \n",
            "0                1.372522         1.207087         1.360451        \n",
            "1                0.850272         1.143556         0.838836        \n",
            "2                0.840800         1.130532         0.829494        \n",
            "3                0.865275         1.126880         0.854007        \n",
            "4                0.853379         1.126235         0.842116        \n",
            "5                0.847912         1.133808         0.836574        \n",
            "Set pseudo-labels on base dataset of type: ConsistentFormatWrapper\n",
            "Pseudo-domain label distribution: {1: 719, 0: 111}\n",
            "\n",
            "==== Domain-invariant feature learning ====\n",
            "epoch            class_loss       dis_loss         contrast_loss    train_acc        valid_acc        target_acc       total_cost_time \n",
            "Initializing projection head for 256 features\n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "0                1.065633         0.000004         7.483785         53.253012        16.088632        14.766686        12.435874       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "1                0.915976         0.000010         5.379797         54.096386        16.377649        15.534554        12.255759       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "2                0.942297         0.000000         7.067926         55.421687        16.570328        15.357354        12.220052       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "3                0.877591         0.000000         5.305712         56.144578        17.148362        15.416421        12.404300       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "4                0.854193         0.000000         4.772618         55.662651        17.052023        15.416421        12.329277       \n",
            "Updated 84 params, skipped 96 mismatched params in teacher update\n",
            "5                0.927594         0.000001         3.979900         55.662651        16.955684        15.416421        12.315062       \n",
            "\n",
            "Calculating h-divergence...\n",
            " H-Divergence: 0.7374, Domain Classifier Acc: 0.6843\n",
            "\n",
            "🎯 Final Target Accuracy: 15.4164\n",
            "\n",
            "📊 Running SHAP explainability...\n",
            "Collecting SHAP background data for GNN...\n",
            "Loader type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
            "Background sample node features shape: torch.Size([8, 1, 200])\n",
            "Evaluation batch node features shape: torch.Size([80, 1, 200])\n",
            "Using first sample as background for GNN\n",
            "Created evaluation batch with 10 graphs\n",
            "Computing SHAP values...\n",
            "/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_pytorch.py:255: UserWarning: unrecognized nn.Module: Flatten\n",
            "  warnings.warn(f\"unrecognized nn.Module: {module_type}\")\n",
            "DeepExplainer failed: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.. Using KernelExplainer\n",
            "100%|##########| 80/80 [00:20<00:00,  3.90it/s]\n",
            "SHAP values shape: (80, 200, 6)\n",
            "Aggregated SHAP values shape: (80, 200)\n",
            "Original SHAP values shape: (80, 200, 6)\n",
            "Original X_eval shape: (80, 1, 200)\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output3/shap_gnn_sample.html\n",
            "✅ Saved summary plot: ./data/train_output3/shap_summary.png\n",
            "✅ Saved signal overlay: ./data/train_output3/shap_overlay.png\n",
            "✅ Saved SHAP heatmap: ./data/train_output3/shap_heatmap.png\n",
            "Evaluating SHAP impact...\n",
            "✅ Saved SHAP values to: ./data/train_output3/shap_values.npy\n",
            "[SHAP] Accuracy Drop: 50.0000\n",
            "[SHAP] Flip Rate: 0.5000\n",
            "[SHAP] Confidence Δ: 0.0291\n",
            "[SHAP] AOPC: 0.0000\n",
            "[SHAP] Entropy: 3.9211\n",
            "[SHAP] Coherence: 0.0404\n",
            "[SHAP] Channel Variance: 0.0000\n",
            "[SHAP] Temporal Entropy: 4.9480\n",
            "[SHAP] Mutual Info: 0.0063\n",
            "[SHAP] PCA Alignment: 0.2955\n",
            "[SHAP] Jaccard (top-10): 0.1111\n",
            "[SHAP] Kendall's Tau: 0.2607\n",
            "[SHAP] Cosine Similarity: 0.3423\n",
            "[4D Plot] RAW Inputs shape: (200, 1, 1), SHAP shape: (6, 1, 200)\n",
            "✅ Saved 4D SHAP plot: ./data/train_output3/shap_4d_scatter.html\n",
            "[Surface] SHAP shape: (80, 6, 1, 200)\n",
            "✅ Saved SHAP surface: ./data/train_output3/shap_4d_surface.html\n",
            "Generating confusion matrix...\n",
            "✅ Confusion matrix saved\n",
            "✅ Training metrics plot saved\n",
            "✅ Domain discrepancy plot saved\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data_dir ./data/ --task cross_people --test_envs 3 --dataset emg --algorithm diversify --alpha1 5.0 --alpha 0.1 --lam 0.0 --local_epoch 3 --max_epoch 1 --lr 0.01 --output ./data/train_output3 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 6 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}