
# GNNIntegrated: EMG-Based Activity Recognition with GNNs, Curriculum Learning, SHAP & Auto-K

## ğŸ“Œ Overview

This project extends the paper OUT-OF-DISTRIBUTION REPRESENTATION LEARNING FOR TIME SERIES CLASSIFICATION at ICLR 2023.

Link- https://paperswithcode.com/paper/generalized-representations-learning-for-time . 

**GNNIntegrated** is a modular deep learning framework for **Human Activity Recognition (HAR)** from **Electromyography (EMG)** data. It leverages **Graph Neural Networks (GNNs)**, along with innovative extensions such as:

- **Curriculum Learning**: Progressive sample learning for better generalization.
- **Automated K Estimation**: Dynamic determination of cluster numbers.
- **SHAP Explainability**: Model interpretation via feature attribution.
- **Graph-Based Learning**: Temporal and structural dependencies through GNNs.

Abstract: We address the challenge of Out-of-Distribution (OOD) generalization in time series by enhancing the DIVERSIFY framework with four key components: Curriculum Learning, Graph Neural Networks, SHAP-based explainability, and Automated K Estimation. These additions improve adaptability, interpretability, and scalability. Our method demonstrates mix performance on real-world datasets like EMG and UCI-HAR, achieving better OOD accuracy and generalization. The integrated framework adapts to unseen domains and offers meaningful insights into model decisions, boosting both usability and reliability.

![image](https://github.com/user-attachments/assets/a5da6a74-70e4-4232-9e5b-153e616d297b)

---

## ğŸ§  Key Features

- **Temporal GCNs** for sequence-aware modeling of EMG signals.
- **Curriculum Learning** pipeline for progressively harder samples.
- **Auto-K Clustering** using Silhouette, CH, DB scores to auto-determine `k`.
- **SHAP Integration** to explain model decisions post-training.
- **Domain Adaptation & Diversification** for robust cross-subject learning.

---

## ğŸ”§ Core Pipelines

### ğŸ‹ï¸ Training Pipeline (`train.py`) 

    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        train.py               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                               â”‚
    â”‚ 1. Argument Parsing           â”‚
    â”‚    â€¢ dataset, model, flags    â”‚
    â”‚                               â”‚
    â”‚ 2. Dataset Preparation        â”‚
    â”‚    â€¢ get_act_dataloader()     â”‚
    â”‚                               â”‚
    â”‚ 3. Graph Construction         â”‚
    â”‚    â€¢ graph_builder.py         â”‚
    â”‚    â€¢ temporal edges           â”‚
    â”‚                               â”‚
    â”‚ 4. Curriculum Learning        â”‚
    â”‚    â€¢ reorder samples          â”‚
    â”‚                               â”‚
    â”‚ 5. Model Initialization       â”‚
    â”‚    â€¢ ActNetwork (GNN)         â”‚
    â”‚    â€¢ domain adaptation opt.   â”‚
    â”‚                               â”‚
    â”‚ 6. Optimization               â”‚
    â”‚    â€¢ multi-component loss     â”‚
    â”‚      - alignment              â”‚
    â”‚      - classification         â”‚
    â”‚      - diversification        â”‚
    â”‚                               â”‚
    â”‚ 7. Logging                    â”‚
    â”‚    â€¢ loss, accuracy           â”‚
    â”‚    â€¢ clustering metrics       â”‚
    â”‚      - Silhouette             â”‚
    â”‚      - Calinski-Harabasz (CH) â”‚
    â”‚      - Davies-Bouldin (DB)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### ğŸ“Š Evaluation Pipeline
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    evaluation pipeline        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                               â”‚
    â”‚ 1. Load Best Model            â”‚
    â”‚    â€¢ resume checkpoint        â”‚
    â”‚                               â”‚
    â”‚ 2. Feature Extraction         â”‚
    â”‚    â€¢ GNN encoder embeddings   â”‚
    â”‚                               â”‚
    â”‚ 3. Auto-k Clustering          â”‚
    â”‚    â€¢ optimal cluster number   â”‚
    â”‚    â€¢ Silhouette, CH, DB       â”‚
    â”‚                               â”‚
    â”‚ 4. Classification             â”‚
    â”‚    â€¢ logistic regression      â”‚
    â”‚    â€¢ or NN classifier         â”‚
    â”‚                               â”‚
    â”‚ 5. SHAP Analysis              â”‚
    â”‚    â€¢ shap_utils.py            â”‚
    â”‚    â€¢ feature attributions     â”‚
    â”‚                               â”‚
    â”‚ 6. Visualization              â”‚
    â”‚    â€¢ confusion matrix         â”‚
    â”‚    â€¢ silhouette plots         â”‚
    â”‚    â€¢ SHAP explanations        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“ File Structure

```
gnnintegrated-main/
â”œâ”€â”€ train.py                  # Main pipeline
â”œâ”€â”€ shap_utils.py            # SHAP explainability
â”œâ”€â”€ env.yml                  # Environment setup
â”œâ”€â”€ alg/                     # Domain adaptation, optimization
â”œâ”€â”€ datautil/                # EMG dataset handling, clustering
â”œâ”€â”€ gnn/                     # Graph construction, GNN models
â”œâ”€â”€ loss/                    # Custom loss functions
â”œâ”€â”€ network/                 # ActNetwork, adversarial nets
â”œâ”€â”€ utils/                   # Arguments, reproducibility
â””â”€â”€ README.md
```

---

## ğŸ“¦ Dataset: EMG

The project is designed around **EMG (Electromyography) data**, processed into temporal sequences and graphs for HAR. Dataset loading is managed by:

```
datautil/actdata/
  â”œâ”€â”€ cross_people.py
  â”œâ”€â”€ util.py
```

Ensure your EMG data is structured or converted accordingly.

Direct link - https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip
---

## ğŸ§ª Extensions Breakdown

### 1. **GNN Backbone**

- Implements **Temporal GCNs** in `gnn/temporal_gcn.py`.
- Graphs constructed using `graph_builder.py`.

### 2. **Curriculum Learning**

- Sample difficulty progression implemented in `get_curriculum_loader()`.

### 3. **Automated K Estimation**

- Uses clustering metrics like:
  - **Silhouette Score**
  - **Calinski-Harabasz Index**
  - **Davies-Bouldin Score**
- Implemented in `train.py` and `datautil/cluster.py`.

### 4. **SHAP Integration**

- Post-hoc interpretability via `shap_utils.py`.
- Applies SHAP on feature embeddings for transparency.

---

## â–¶ï¸ How to Run

### 1. Install Environment

```bash
conda env create -f env.yml
conda activate base
```
### 2. Install Dependencies
```bash
# Clean up conflicting packages
pip uninstall -y thinc spacy accelerate peft fastai sentence-transformers

# Install core requirements
pip install numpy==1.26.3 --upgrade torch_geometric

pip install fastdtw scipy

pip install scikit-learn scipy torch torchvision shap pandas matplotlib seaborn tqdm plotly
```
### 3. Dataset download
EMG (electromyography)

     # Download the dataset
     wget https://wjdcloud.blob.core.windows.net/dataset/diversity_emg.zip
     unzip diversity_emg.zip && mv emg data/
     
     # Create necessary directories
     mkdir -p ./data/train_output/act/
     
     mkdir -p ./data/emg
     mv emg/* ./data/emg
     
### 4. Train the Model

   Basic Execution
```bash
python train.py --data_dir ./data/ --task cross_people --test_envs 0 --dataset emg --algorithm diversify --alpha1 1.0 --alpha 1.0 --lam 0.0 --local_epoch 3 --max_epoch 3 --lr 0.0001 --output ./train_output --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 2 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 2

python train.py --data_dir ./data/ --task cross_people --test_envs 1 --dataset emg --algorithm diversify --alpha1 0.1 --alpha 10.0 --lam 0.0 --local_epoch 2 --max_epoch 5 --lr 0.01 --output ./data/train_output1 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 3 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 3

python train.py --data_dir ./data/ --task cross_people --test_envs 2 --dataset emg --algorithm diversify --alpha1 0.5 --alpha 21.5 --lam 0.0 --local_epoch 4 --max_epoch 7 --lr 0.01 --output ./data/train_output2 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.3 --automated_k --curriculum --CL_PHASE_EPOCHS 4 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 4

python train.py --data_dir ./data/ --task cross_people --test_envs 3 --dataset emg --algorithm diversify --alpha1 5.0 --alpha 0.1 --lam 0.0 --local_epoch 3 --max_epoch 9 --lr 0.01 --output ./data/train_output3 --batch_size 64 --weight_decay 1e-3 --dropout 0.5 --label_smoothing 0.1 --automated_k --curriculum --CL_PHASE_EPOCHS 5 --enable_shap --use_gnn --gnn_hidden_dim 128 --gnn_output_dim 256 --gnn_pretrain_epochs 5
```


Optional flags (see `utils/util.py::get_args()`):
- `--curriculum`
- `--enable_shap`
- `--gnn_hidden_dim`
- `--domain_adapt`

---

## ğŸ§¾ Outputs and Artifacts

- âœ… Trained model weights
- ğŸ“‰ Loss and accuracy logs
- ğŸ“Š Clustering plots
- ğŸ“ˆ Confusion matrices
- ğŸ” SHAP explanations
- ğŸ“ Embedding files for downstream tasks

---

## ğŸ“ˆ Analysis & Visualization

- **Clustering Evaluation**:
  - Automatic `k` tuning
  - CH, DB, and Silhouette plotted
- **Classification Accuracy**:
  - Per-class metrics and confusion matrices
- **SHAP Analysis**:
  - Local/global feature importance
  - Interpretable visual outputs

---

## Limitations and Future Prospects

GNNs proved ineffective for cross-subject EMG classification due to EMG's lack of inherent graph structure, leading to catastrophic training failure and ~16% accuracy. Exploding losses and poor domain alignment indicated architectural mismatch. Latent clustering was unstable, failing domain generalization. In contrast, CNNs like ResNet or TCN exploit EMGâ€™s temporal patterns effectively. GNNs fundamentally require data with inherent graph topology; for tabular or sequential data like EMG, a meaningful graph must first be manually and carefully constructed. Without this, GNNs cannot learn effectively. We recommend removing --use_gnn and using EMG-specific preprocessing with CNNs and domain adaptation methods (e.g., Deep CORAL). GNN integration would require extensive tuning over 3+ months, making CNNs the practical(for short period projects), accurate (80â€“85%) solution.

---

## ğŸªª License

This project is free for academic and commercial use with attribution.

            @misc{extdd2025,
              title={GNNIntegrated: EMG-Based Activity Recognition with GNNs, Curriculum Learning, SHAP & Auto-K},
              author={Rishabh Gupta et al.},
              year={2025},
              note={https://github.com/rishabharizona/gnnintegrated}
            }

## Contact

E-mail- rishabhgupta8218@gmail.com
